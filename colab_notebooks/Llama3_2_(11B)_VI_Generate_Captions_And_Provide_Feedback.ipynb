{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaRHoj708-4T"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcNycGH68-4Y"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hn52xzQB8-4Z"
      },
      "source": [
        "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
        "\n",
        "Read our **[Gemma 3N Guide](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtfvYiol8-4a"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "ZezOAqrnKn-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "XB9BOQKS8-4a"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spaces"
      ],
      "metadata": {
        "id": "E5VmqdU6GbIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g59-MoJv8-4c"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
        "import torch\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\", # Llama 3.2 vision support\n",
        "    \"unsloth/Llama-3.2-11B-Vision-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit\", # Can fit in a 80GB card!\n",
        "    \"unsloth/Llama-3.2-90B-Vision-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/Pixtral-12B-2409-bnb-4bit\",              # Pixtral fits in 16GB!\n",
        "    \"unsloth/Pixtral-12B-Base-2409-bnb-4bit\",         # Pixtral base model\n",
        "\n",
        "    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",          # Qwen2 VL support\n",
        "    \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Qwen2-VL-72B-Instruct-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit\",      # Any Llava variant works!\n",
        "    \"unsloth/llava-1.5-7b-hf-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    \"unsloth/Llama-3.2-11B-Vision-Instruct\",\n",
        "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters for parameter efficient finetuning - this allows us to only efficiently train 1% of all parameters.\n",
        "\n",
        "**[NEW]** We also support finetuning ONLY the vision part of the model, or ONLY the language part. Or you can select both! You can also select to finetune the attention or the MLP layers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "334f66fc-2e6e-4055-a934-789470f54bb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Making `model.base_model.model.model.vision_model.transformer` require gradients\n"
          ]
        }
      ],
      "source": [
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = True, # False if not finetuning vision layers\n",
        "    finetune_language_layers   = True, # False if not finetuning language layers\n",
        "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
        "    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
        "\n",
        "    r = 16,           # The larger, the higher the accuracy, but might overfit\n",
        "    lora_alpha = 16,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spaces\n",
        "from datasets import load_dataset, Image\n",
        "\n",
        "# Login using e.g. `huggingface-cli login` to access this dataset\n",
        "indian_monuments_ds = load_dataset(\"AIMLOps-C4-G16/indian_monuments\")"
      ],
      "metadata": {
        "id": "findWxOlGXeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#indian_festivals_ds = load_dataset(\"AIMLOps-C4-G16/IndianFestivals\")"
      ],
      "metadata": {
        "id": "Q0r8xFbmHNtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1W2Qhsz6rUT"
      },
      "source": [
        "Let's take a look at the dataset, and check what the 1st example shows:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indian_monuments_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mPN3XxGH4Dr",
        "outputId": "1a76db69-426d-474d-c3ad-c7ebf053c771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['image'],\n",
              "        num_rows: 148\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(indian_monuments_ds['train'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InWHSmKLSUpD",
        "outputId": "b02b13f3-6dca-4b90-cfe5-9c5ea2e080d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "148"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indian_monuments_ds['train'][0][\"image\"]"
      ],
      "metadata": {
        "id": "q7AdliU_I7BU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"AIMLOps-C4-G16/indian_monuments\", split=\"train\").cast_column(\"image\", Image(decode=False))\n",
        "dataset[0][\"image\"]"
      ],
      "metadata": {
        "id": "R7EhPT_JLVE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_image_names = []\n",
        "for i in range(len(dataset['train'])):\n",
        "  list_of_image_names.append(((dataset['train'][i][\"image\"])['path']).split('/')[-1])"
      ],
      "metadata": {
        "id": "2T_1WY_bLkHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FecKS-dA82f5"
      },
      "source": [
        "Before we do any finetuning, maybe the vision model already knows how to analyse the images? Let's check if this is the case!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FastVisionModel.for_inference(model) # Enable for inference!\n",
        "\n",
        "image = indian_monuments_ds[\"train\"][0][\"image\"]\n",
        "instruction = \"Identify the monument with a short caption in less than 10 words\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": [\n",
        "        {\"type\": \"image\"},\n",
        "        {\"type\": \"text\", \"text\": instruction}\n",
        "    ]}\n",
        "]\n",
        "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n",
        "inputs = tokenizer(\n",
        "    image,\n",
        "    input_text,\n",
        "    add_special_tokens = False,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
        "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS1iIa2lNvDi",
        "outputId": "5234302a-2255-4179-8fd0-a2abea43556f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The monument is an ornate, Indian temple with intricate details.<|eot_id|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model!\n",
        "We use `min_p = 0.1` and `temperature = 1.5`. Read this [Tweet](https://x.com/menhguin/status/1826132708508213629) for more information on why."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate 4 captions per image and write to a file"
      ],
      "metadata": {
        "id": "JKuOCv9rggO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Captions file will be in this format:\n",
        "\n",
        "img_name \\t caption 0 \\t caption 1 \\t caption 2 \\t caption 3 \\n\n",
        "'''"
      ],
      "metadata": {
        "id": "L6bsLxLQtm41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_captions_per_image = 4\n",
        "\n",
        "def generate_captions():\n",
        "  with open('llama3.2_11b_vi_monuments_captions.txt', 'w') as f:\n",
        "    for i in range(len(indian_monuments_ds['train'])):\n",
        "      output = []\n",
        "      for j in range(num_captions_per_image):\n",
        "        image = indian_monuments_ds['train'][i][\"image\"]\n",
        "        inputs = tokenizer(\n",
        "                     image,\n",
        "                     input_text,\n",
        "                     add_special_tokens = False,\n",
        "                     return_tensors = \"pt\",).to(\"cuda\")\n",
        "        text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "        _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
        "                                   use_cache = True, temperature = 1.5, min_p = 0.1)\n",
        "        output[j] = \" \".join(_)\n",
        "      f.write(list_of_image_names[i] + \"\\t\" + output[0] + \"\\t\" + output[1] + \"\\t\" + output[2] + \"\\t\" + output[3] + \"\\n\")"
      ],
      "metadata": {
        "id": "mJhEO23SSwIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_captions()"
      ],
      "metadata": {
        "id": "dtfgOkENaF81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download llama3.2_11b_vi_monuments_captions.txt\n",
        "from google.colab import files\n",
        "files.download('llama3.2_11b_vi_monuments_captions.txt')"
      ],
      "metadata": {
        "id": "Uov1yIqMTOXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Feedback file will be in this format\n",
        "\n",
        "img_name \\t caption 0 \\t caption 1 \\t caption 2 \\t caption 3 \\t best_caption_number(-1,0,1,2,3) \\t alternate_caption \\n\n",
        "'''"
      ],
      "metadata": {
        "id": "7C0a23EtMpkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "def get_next_image_and_captions():\n",
        "  with open('llama3.2_11b_vi_monuments_captions.txt') as f:\n",
        "    for i, line in enumerate(f):\n",
        "      if i == count:\n",
        "        #img_name is .jpg file name\n",
        "        img_name, c0,c1,c2,c3 = line.split('\\t')\n",
        "        #image is actual path to image file /root/.cache/huggingface/datasets/downloads/extracted/\n",
        "        image = indian_monuments_ds['train'][count][\"image\"]\n",
        "        count += 1\n",
        "        return image, c0, c1, c2, c3"
      ],
      "metadata": {
        "id": "4Frkk5_4MfmA"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thanks_message = \"Done\"\n",
        "def run_rlhf(c0, c1, c2, c3, best_caption_number, alternate_caption):\n",
        "  if c0 is not None:\n",
        "    with open('rlhf_llama3.2_11b_monuments.txt', 'w') as f:\n",
        "      f.write(list_of_image_names[count-1] + \"\\t\" + c0 + \"\\t\" + c1 + \"\\t\" + c2 + \"\\t\" + c3 + \"\\t\" + best_caption_number + \"\\t\" + alternate_caption + \"\\n\")\n",
        "  return thanks_message, get_next_image_and_captions()"
      ],
      "metadata": {
        "id": "3uvTXyx2MjI2"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "css = \"\"\"\n",
        "  #output {\n",
        "    height: 500px;\n",
        "    overflow: auto;\n",
        "    border: 1px solid #ccc;\n",
        "  }\n",
        "\"\"\"\n",
        "rlhf_btn = gr.Button(\"Ok, Next Image\")\n",
        "input_img = gr.Image(label=\"Input Picture\")\n",
        "output_img = gr.Image(label=\"Input Picture\")\n",
        "c0 = gr.Textbox(label=\"Caption 0\")\n",
        "c1 = gr.Textbox(label=\"Caption 1\")\n",
        "c2 = gr.Textbox(label=\"Caption 2\")\n",
        "c3 = gr.Textbox(label=\"Caption 3\")\n",
        "best_caption_number = gr.Textbox(label=\"Choose best caption number -1(None),0,1,2,3\")\n",
        "alternate_caption = gr.Textbox(label=\"Your suggestion for an alternate caption\")\n",
        "response_output = gr.Textbox(label=\"Response\") # Add a textbox for the response\n",
        "\n",
        "with gr.Blocks(css=css) as demo:\n",
        "    gr.Markdown(\"RLHF\")\n",
        "    with gr.Tab(label=\"Real or Kidding?\"):\n",
        "        with gr.Row():\n",
        "          with gr.Column():\n",
        "            rlhf_btn.render()\n",
        "            rlhf_btn.click(run_rlhf, [c0, c1, c2, c3, best_caption_number, alternate_caption],[response_output, output_img, c0, c1, c2, c3])\n",
        "            @gr.render(triggers=[rlhf_btn.click])\n",
        "            def rlhf():\n",
        "              output_img.render()\n",
        "              c0.render()\n",
        "              c1.render()\n",
        "              c2.render()\n",
        "              c3.render()\n",
        "              # Display the image using the path from the state\n",
        "          with gr.Column():\n",
        "            best_caption_number.render()\n",
        "            alternate_caption.render()\n",
        "            response_output.render()\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "NO9-Nz2-Mbvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhAzQzyn8-4l"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}